from langchain.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_openai import ChatOpenAI
from app.schemas.response import QueryResponse, Document as APIDocument


def build_rag_chain(vector_store):
    # Prompt con variable 'context' porque es lo que recibe el LLM luego del formato
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "Eres un asistente legal experto en el C贸digo de Tr谩nsito Boliviano. "
                "Utiliza 煤nicamente la informaci贸n proporcionada a continuaci贸n para responder. "
                "Si no encuentras el monto exacto de la multa, pero puedes inferir si algo est谩 prohibido o sancionado, expl铆calo claramente. "
                "Si no hay informaci贸n 煤til, responde: 'No se encontr贸 informaci贸n suficiente...'",
            ),
            (
                "human",
                "{context}",
            ),  # Esta variable es llenada por LangChain con los documentos recuperados
        ]
    )

    # LLM configurado
    llm = ChatOpenAI(temperature=0.7)

    # Crea la cadena que combina documentos y los convierte en texto
    combine_chain = create_stuff_documents_chain(llm, prompt)

    # Crea el retriever con el vector store
    retriever = vector_store.as_retriever()

    # Une el retriever con la cadena de combinaci贸n (RAG)
    return create_retrieval_chain(retriever, combine_chain)


def process_query_with_retrieval(query: str, vector_store) -> QueryResponse:
    """
    Usa el pipeline RAG (retrieval + generaci贸n) con vector store ya cargado.
    Devuelve una respuesta estructurada con texto y art铆culos fuente usados.
    """
    print(" [RAG] Procesando consulta:", query)

    # Construye el pipeline RAG
    rag_chain = build_rag_chain(vector_store)

    # Ejecuta el pipeline pasando la consulta como 'input'
    result = rag_chain.invoke(
        {"input": query}
    )  # 隆IMPORTANTE! 'input' es la entrada esperada

    # Extraer contenido de los fragmentos/art铆culos como fuentes
    retrieved_docs = vector_store.as_retriever().invoke(query)
    # Transformar LangChain.Document a tu esquema Document esperado por el frontend
    documents_context = [
        APIDocument(
            id=doc.metadata.get("id", "sin_id"),
            metadata=doc.metadata,
            page_content=doc.page_content,
            type="document",
        )
        for doc in retrieved_docs
    ]

    # Debug: imprime el contexto enviado al LLM
    context = "\n\n".join(
        [
            f"Fuente: {doc.metadata.get('source', 'desconocido')}\n{doc.page_content}"
            for doc in retrieved_docs
        ]
    )
    print("Ь Contexto completo enviado al LLM:\n", context)

    # Devuelve la respuesta estructurada
    return QueryResponse(
        answer=result.get("answer", "") if isinstance(result, dict) else str(result),
        context=documents_context,
    )
